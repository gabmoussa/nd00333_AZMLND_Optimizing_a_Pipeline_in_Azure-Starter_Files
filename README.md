# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data about [bank marketing](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv) campaigns of a banking institution. The dataset contains 21 attributes of clients' information collected including (age, job, marital status, education, contact type, etc...) and we seek to predict if the client will subscribe to a term deposit (variable y).

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was a VotingEnsemble algorithm for the AutoML run with an accuracy of 0.91563 (91.730%). This outperformed the Hyperparameter tuned Scikit-Learn LogicRegression algorithm which had the best accuracy of 0.90932 (90.932%).


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

![architecture](creating-and-optimizing-an-ml-pipeline.png)

- In the Scikit-learn pipeline, we start by setting up a train.py script in which we create a TabularDataset (data source: [bank marketing dataset](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv)) using TabularDatasetFactory.
- We cleaned the dataset dropping all row with null entries and then performed one-hot encoding for the attributes such as (marital, job, default, housing, loan, etc...). 
- We used the column `y` (which refers to - has the client subscribed to a term deposit? ) as the label set.
- The cleaned data is then split into train and test sets using the `train_test_split` method from the Scikit-learn library.
- The dataset is evaluated with the logistic regression from Scikit-learn.
- We now use HyperDrive (in the notebook `udacity-project.ipynb`) to find optimal hyperparameters for the logistic regression.
-  We are passing a parameter space below into a RandomParameterSampling method from which HyperDrive will select the best parameters for `--C` and `--max_iter`.
``` 
{
    "--C": uniform(0.80, 1.0),
    "--max_iter" : chioce(20, 60, 100, 120)
} 
```  
- In addiion, we setup erly stopping policy with `BanditPolicy` using `evaluation_interval=2`, and `slack_factor=0.1`. 
- Again, we create an SKLearn estimator for use with train.py then create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.

**What are the benefits of the parameter sampler you chose?**

We used the RandomParameterSampling here since parameter values are chosen from a set of discrete values or distribution over a continuous range. This algorithm picks up random values from the parameter space and again it supports early stopping policy. Unlike the GridParameterSampling which uses parameter values choosing from discrete values and the algorithm is very exhaustive. Again, unlike the BayesianParameterSampling which tries to intelligently pick the next sample of hyperparameters, based on how the previous samples performed, such that the new sample improves the reported primary metric.

**What are the benefits of the early stopping policy you chose?**

We use early stopping policy with the BanditPolicy alorithm, evalating the accuracy of the model and set to stop we get an accuracy below 0.1 after every two iterations. The Bandit Policy helps in that any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

- The best model in the AutoML run is the VotingEnsemble algorithm and here the Explainating shows that the `duration` attribute is the most important. 

- Some hyperparameters genrated here are; l1_ratio=0.3877510 (L1 regulatrization technique is used thus lasso regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function), learning_rate='invscalin', loss='log'(Logarithmic loss is applied thus quantifies the accuracy the classifier by penalising false classifications), max_iter=1000 (The maximum number of passes over the training data).



## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

- AutoML run produces a model with the algorithm VotingEnsemble which has an accuracy of 91.730% while hyperdrive run had an accuracy of 90.932% using Logistic regression with tuned parameters. The difference in accuracy is as a result of the ability of AutoML to search for the best algorithm and self-optimizing its parameters while in HyperDrive and algorithm s chosen manually and optimizing is done automatically by HyprDrive selecting the best run from a given set of parameters.
- AutoML and HyperDrive have similar architectures but a difference in configurations. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- Class imbalacing is detected in the model during AutoML run and it will be good to fix this in the future. Imbalanced data can lead to a falsely perceived positive effect of a model's accuracy because the input data has a bias towards one class.
- More values could be added to the Random Parameters Sampler so that better parameters may be used to train the model.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**

I used the delete method (compute_target.delete()) in the Notebook to delete th compute cluster

